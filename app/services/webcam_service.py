import contextlib
import io
import uuid
import cv2
import torch
import time
import os
import boto3
import numpy as np
import queue
from collections import deque
from ultralytics import YOLO
from fastapi.responses import JSONResponse
from config import config
from app.services.cnn3d import Deeper3DCNN_Gray

# âœ… ì‹¤ì‹œê°„ ë˜ëŠ” íŒŒì¼ ì‹¤í–‰ ëª¨ë“œ
USE_VIDEO_FILE = True  # ğŸ”„ ì˜ìƒ íŒŒì¼ ê¸°ë°˜ ì‹¤í–‰ìœ¼ë¡œ ì„¤ì •
VIDEO_FILE_PATH = "app/services/test/pulling.mp4"  # ğŸ”„ ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš© mp4 íŒŒì¼ ê²½ë¡œ

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ“± ì‹¤í–‰ ì¥ì¹˜: {device}")
print("ğŸ® í˜„ì¬ ëª¨ë“œ: íŒŒì¼ ì¬ìƒ ê¸°ë°˜" if USE_VIDEO_FILE else "ğŸ“¸ í˜„ì¬ ëª¨ë“œ: ì‹¤ì‹œê°„ ì¹´ë©”ë¼")

# âœ… ëª¨ë¸ ë¡œë“œ
model = YOLO("yolov8n.pt", verbose=False).to(device)

from ultralytics.utils import LOGGER # yolo ëª¨ë¸ ë¡œê·¸ ì œê±° ìœ„ì¹˜ ê³ ì • 
LOGGER.setLevel("WARNING")  # ë˜ëŠ” "ERROR" ë˜ëŠ” "CRITICAL"



cnn_model = Deeper3DCNN_Gray().to(device)
cnn_model.load_state_dict(torch.load("app/services/deeper3dcnn_gray3.pth", map_location=device))
cnn_model.eval()

# âœ… ìƒíƒœ
last_person_detected = {}
buffer_full_printed = {}
frame_buffers = {}
buffer_start_times = {}
video_paths = {}
latest_video_url = None
alerts = set()
active_cameras = {}
last_detection_time = {}
last_upload_time = {} # ì „ì—­ì— ì €ì¥ ê°„ê²© ì œí•œ ë³€ìˆ˜ ì¶”ê°€
UPLOAD_COOLDOWN = 5  # ì´ˆ ë‹¨ìœ„, ì˜ˆ: 10ì´ˆì— í•œ ë²ˆë§Œ ì—…ë¡œë“œ í—ˆìš©


if not os.path.exists("recordings"):
    os.makedirs("recordings")

# âœ… ë¶„ì„ í† ì™€
frame_analysis_queue = queue.Queue()

s3_client = boto3.client(
    "s3",
    aws_access_key_id=config.ACCESS_KEY,
    aws_secret_access_key=config.SECRET_KEY,
    region_name=config.BUCKET_REGION,
)

def upload_to_s3(file_path, camera_index):
    print("ğŸ“¤ upload_to_s3() ì‹¤í–‰ì¤‘!")
    global latest_video_url
    try:
        filename = os.path.basename(file_path)
        s3_key = f"{config.BUCKET_DIRECTORY}/{filename}"
        with open(file_path, "rb") as file:
            s3_client.upload_fileobj(
                file, config.BUCKET_NAME, s3_key,
                ExtraArgs={
                    'ACL': 'public-read',
                    'ContentDisposition': 'inline',
                    'ContentType': 'video/mp4'
                }
            )
        s3_url = f"https://{config.BUCKET_NAME}.s3.{config.BUCKET_REGION}.amazonaws.com/{s3_key}"
        video_paths[camera_index] = s3_url
        latest_video_url = s3_url
        print(f"âœ… S3 ì—…ë¡œë“œ ì„±ê³µ: {s3_url}")
        os.remove(file_path)
        return s3_url
    except Exception as e:
        print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_next_video_filename():
    print("ğŸ“œ get_next_video_filename() ì‹¤í–‰ì¤‘!")
    timestamp = int(time.time())
    unique_id = str(uuid.uuid4())[:8]
    return os.path.join("recordings", f"{timestamp}_{unique_id}.mp4")


def sample_16_frames(frames):
    print("ğŸŸ sample_16_frames() ì‹¤í–‰ì¤‘!")
    if len(frames) < 16:
        return []
    indices = np.linspace(0, len(frames) - 1, 16, dtype=int)
    return [frames[i] for i in indices]

def preprocess_frames(frames):
    print("ğŸ§ª preprocess_frames() ì‹¤í–‰ì¤‘!")
    gray_frames = [cv2.cvtColor(cv2.resize(f, (112, 112)), cv2.COLOR_BGR2GRAY) for f in frames]
    gray_frames = np.stack(gray_frames, axis=0)          # (16, 112, 112)
    gray_frames = np.expand_dims(gray_frames, axis=0)    # (1, 16, 112, 112)
    gray_frames = np.expand_dims(gray_frames, axis=1)    # âœ… (1, 1, 16, 112, 112) â† ì—¬ê¸° ì¶”ê°€!
    return torch.FloatTensor(gray_frames).to(device) / 255.0

def detect_people(frame, camera_index):
    global last_person_detected
    person_detected = False

    f = io.StringIO()
    with contextlib.redirect_stdout(f):
        results = model(frame)

    for result in results:
        for box in result.boxes.data:
            class_id = int(box[5])
            confidence = float(box[4])
            if model.names[class_id] == "person" and confidence > 0.3:
                person_detected = True
                break

    # ğŸ”„ ìƒíƒœ ë°”ë€Œì—ˆì„ ë•Œë§Œ ì¶œë ¥
    if camera_index not in last_person_detected or last_person_detected[camera_index] != person_detected:
        last_person_detected[camera_index] = person_detected
        if person_detected:
            print(f"ğŸ§â€â™‚ï¸ ì‚¬ëŒ ê°ì§€ë¨! (CAM{camera_index+1})")
        else:
            print(f"ğŸ™ˆ ì‚¬ëŒ ì•ˆ ë³´ì„! (CAM{camera_index+1})")

    return person_detected


def save_video(frames, filename):
    print("ğŸ“ save_video() ì‹¤í–‰ì¤‘!")

    fourcc = cv2.VideoWriter_fourcc(*"mp4v")  # âœ… ì•ˆì •ì  ì½”ë±
    if len(frames) == 0:
        print("âŒ ì €ì¥í•  í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    h, w = frames[0].shape[:2]
    print(f"ğŸ–¼ ì €ì¥í•  ì˜ìƒ í¬ê¸°: {w}x{h}, ì´ {len(frames)} í”„ë ˆì„")

    out = cv2.VideoWriter(filename, fourcc, 15, (w, h))
    for i, frame in enumerate(frames):
        out.write(frame)
    out.release()

def analyze_buffers(original_fps, camera_index):
    global buffer_full_printed, last_upload_time

    now = time.time()

    last_upload = last_upload_time.get(camera_index, 0)
    if now - last_upload < UPLOAD_COOLDOWN:
        return

    for buffer_id in [0, 1, 2]:
        key = (camera_index, buffer_id)
        buffer_frames = list(frame_buffers.get(key, []))

        # âœ… CNN ë¶„ì„ ì¡°ê±´: í”„ë ˆì„ì´ 45ê°œ ë‹¤ ì°¨ì•¼ í•¨
        if len(buffer_frames) < 45:
            continue

        # âœ… ë””ë²„ê¹…ìš©: ë²„í¼ ê½‰ ì°¼ì„ ë•Œ 1íšŒ ì¶œë ¥
        if not buffer_full_printed.get(key, False):
            print(f"âœ… {key} ë²„í¼ê°€ ë‹¤ ì°¼ì–´ìš”! CNN ë¶„ì„ ì‹œì‘")
            buffer_full_printed[key] = True
        else:
            continue  # ì´ë¯¸ ì²˜ë¦¬ëœ ë²„í¼ëŠ” ìŠ¤í‚µ

        # ğŸ¯ CNN ë¶„ì„ìš© 16í”„ë ˆì„ ìƒ˜í”Œë§
        clip_for_cnn = sample_16_frames(buffer_frames)
        if len(clip_for_cnn) < 16:
            continue

        input_tensor = preprocess_frames(clip_for_cnn)
        with torch.no_grad():
            output = cnn_model(input_tensor)
        pred = torch.argmax(output, dim=1).item()

        print(f"ğŸ§  CNN ê²°ê³¼: buffer{buffer_id} â†’ pred={pred}")
        if pred == 1:
            print(f"ğŸš¨ í­ë ¥ ê°ì§€! buffer{buffer_id} ì „ì²´ ì €ì¥")

            filename = get_next_video_filename()
            save_video(buffer_frames, filename)
            upload_to_s3(filename, camera_index)

            alerts.add(f"CAM{camera_index+1}")
            last_detection_time[camera_index] = now
            last_upload_time[camera_index] = now

def background_analyzer():
    print("ğŸ§µ background_analyzer() ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘!")
    step = 2
    buffer_duration = 5
    fps_assumed = 15
    buffer_ids = [0, 1, 2]
    for cam in range(5):
        for bid in buffer_ids:
            frame_buffers[(cam, bid)] = deque(maxlen=fps_assumed * buffer_duration)
            buffer_start_times[(cam, bid)] = None

    while True:
        if frame_analysis_queue.empty():
            time.sleep(0.01)
            continue

        frame, fps, camera_index = frame_analysis_queue.get()
        now = time.time()

        if not detect_people(frame, camera_index):
            for bid in buffer_ids:
                buffer_start_times[(camera_index, bid)] = None
            continue

        for bid in buffer_ids:
            key = (camera_index, bid)
            offset = bid * step
            start_time = buffer_start_times.get(key)
            if start_time is None or now - start_time > buffer_duration:
                buffer_start_times[key] = now
                frame_buffers[key].clear()
            frame_buffers[key].append(frame)

        analyze_buffers(fps, camera_index)

def process_video_stream(camera_index=0):
    print("ğŸ© process_video_stream() ì‹¤í–‰ì¤‘!")
    if USE_VIDEO_FILE:
        source = VIDEO_FILE_PATH
        cap = cv2.VideoCapture(source)
    else:
        source = camera_index
        cap = cv2.VideoCapture(source, cv2.CAP_ANY)

    if not cap.isOpened():
        print("âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨!")
        return

    fps = cap.get(cv2.CAP_PROP_FPS) or 15

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_analysis_queue.put((frame.copy(), fps, camera_index))

        _, buffer = cv2.imencode(".jpg", frame)
        yield (b"--frame\r\n"
               b"Content-Type: image/jpeg\r\n\r\n" + buffer.tobytes() + b"\r\n")

        # âœ… ì˜ìƒ íŒŒì¼ì¼ ë•ŒëŠ” ë¹ ë¥´ê²Œ ì²˜ë¦¬ (sleep ì œê±°)
        if not USE_VIDEO_FILE:
            time.sleep(1 / fps if fps > 0 else 1 / 15)

    cap.release()


def get_latest_video_url():
    print("ğŸ”— get_latest_video_url() ì‹¤í–‰ì¤‘!")
    global latest_video_url
    return JSONResponse({"video_url": latest_video_url if latest_video_url else None})

def get_available_cameras():
    print("ğŸ“· get_available_cameras() ì‹¤í–‰ì¤‘!")
    available = []
    index = 0
    while True:
        cap = cv2.VideoCapture(index, cv2.CAP_ANY)
        if not cap.isOpened():
            break
        available.append(f"CAM{index+1}")
        cap.release()
        index += 1
    return available

def get_alerts():
    print("ğŸš¨ get_alerts() ì‹¤í–‰ì¤‘!")
    return {"alerts": list(alerts)}
